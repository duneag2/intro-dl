{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duneag2/intro-dl/blob/main/DLMATHon/uh/dlmathon2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nci3cj0-hpnp",
        "outputId": "32a463bf-0aed-48e6-9921-69ead311ca85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyheif\n",
            "  Downloading pyheif-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyheif) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->pyheif) (2.22)\n",
            "Downloading pyheif-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/9.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/9.8 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/9.8 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyheif\n",
            "Successfully installed pyheif-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyheif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WtN5FT3Z3qB",
        "outputId": "d826b761-1134-4ba0-bc05-67473711326a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf2V9fgvagcl",
        "outputId": "6042a9f9-6888-44e4-bdf2-70e1d6ea86c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace /content/drive/MyDrive/ddubuk/test/ddubuk/IMG_0974.HEIC? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/ddubuk/test/ddubuk/IMG_1847.HEIC? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/ddubuk/test/ddubuk/IMG_1848.HEIC? [y]es, [n]o, [A]ll, [N]one, [r]ename: r\n",
            "new name: "
          ]
        }
      ],
      "source": [
        "! unzip -qq '/content/drive/MyDrive/ddubuk_cls.zip' -d '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HEh8OTJT7Jv",
        "outputId": "2e74f843-e787-427d-b38d-b211a887f789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ddubuk\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/ddubuk/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePM7xRskdvnp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "from torchvision import datasets, models, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "import random\n",
        "import pyheif\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPieqX2tT4f0",
        "outputId": "5af46661-d033-45f9-cd1e-030c9590bc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PyTorch version: 2.3.1+cu121  Device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkaBGxzdhCpo"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "train_dir = '/content/drive/MyDrive/ddubuk/train'\n",
        "test_dir = '/content/drive/MyDrive/ddubuk/test'\n",
        "\n",
        "# Convert images to a common format (JPEG)\n",
        "def convert_images_to_jpg(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            filepath = os.path.join(root, file)\n",
        "            new_filepath = os.path.splitext(filepath)[0] + '.jpg'\n",
        "\n",
        "            if file.lower().endswith('.heic'):\n",
        "                try:\n",
        "                    heif_file = pyheif.read(filepath)\n",
        "                    image = Image.frombytes(\n",
        "                        heif_file.mode,\n",
        "                        heif_file.size,\n",
        "                        heif_file.data,\n",
        "                        \"raw\",\n",
        "                        heif_file.mode,\n",
        "                        heif_file.stride,\n",
        "                    )\n",
        "                    image = image.convert('RGB')\n",
        "                    image.save(new_filepath, \"JPEG\")\n",
        "                    os.remove(filepath)\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not convert {filepath}: {e}\")\n",
        "\n",
        "            elif not file.lower().endswith('.jpg'):\n",
        "                try:\n",
        "                    img = Image.open(filepath)\n",
        "                    img = img.convert('RGB')\n",
        "                    img.save(new_filepath, 'JPEG')\n",
        "                    os.remove(filepath)\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not convert {filepath}: {e}\")\n",
        "\n",
        "convert_images_to_jpg(train_dir)\n",
        "convert_images_to_jpg(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ychp9phhIXB"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2cPg8XehMXi",
        "outputId": "cd9cc085-5283-4ad6-f9b3-d1300bd9df93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "# Load data from folders\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
        "\n",
        "# Split the train_dataset into train and validation datasets\n",
        "train_indices, val_indices = train_test_split(list(range(len(train_dataset))), test_size=0.2, stratify=train_dataset.targets)\n",
        "\n",
        "train_set = Subset(train_dataset, train_indices)\n",
        "val_set = Subset(train_dataset, val_indices)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMUcmD8KhMUh"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained ResNet50 model\n",
        "model_ft = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the final fully connected layer to match the number of classes (ddubuk and not_ddubuk)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)  # 2 classes: ddubuk and not_ddubuk\n",
        "\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THsxR0fvhMRk"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnoU853HhMIz",
        "outputId": "15f30e51-bd5e-4863-86dd-d03d1648152e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 0.4502 Acc: 0.6538\n",
            "val Loss: 20.0485 Acc: 0.2000\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.6596 Acc: 0.9103\n",
            "val Loss: 2.9183 Acc: 0.4500\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.1354 Acc: 0.9615\n",
            "val Loss: 10.1592 Acc: 0.5000\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.1618 Acc: 0.9103\n",
            "val Loss: 4.8357 Acc: 0.8500\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.1082 Acc: 0.9615\n",
            "val Loss: 7.0270 Acc: 0.8500\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.1956 Acc: 0.8974\n",
            "val Loss: 5.1192 Acc: 0.8500\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.1605 Acc: 0.9487\n",
            "val Loss: 2.5069 Acc: 0.3500\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.0961 Acc: 0.9744\n",
            "val Loss: 1.4982 Acc: 0.6500\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.1262 Acc: 0.9359\n",
            "val Loss: 0.5161 Acc: 0.7500\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.0931 Acc: 0.9872\n",
            "val Loss: 0.4234 Acc: 0.8000\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.0852 Acc: 0.9744\n",
            "val Loss: 0.1138 Acc: 0.9500\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.1167 Acc: 0.9615\n",
            "val Loss: 0.2211 Acc: 0.9000\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.0621 Acc: 0.9744\n",
            "val Loss: 0.9087 Acc: 0.8000\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.0462 Acc: 0.9744\n",
            "val Loss: 0.4689 Acc: 0.8500\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.0481 Acc: 0.9744\n",
            "val Loss: 0.3286 Acc: 0.9000\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.0347 Acc: 0.9872\n",
            "val Loss: 0.7350 Acc: 0.8500\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.0862 Acc: 0.9872\n",
            "val Loss: 0.4580 Acc: 0.9000\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.0304 Acc: 0.9872\n",
            "val Loss: 0.5983 Acc: 0.8500\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.0882 Acc: 0.9487\n",
            "val Loss: 0.4563 Acc: 0.8500\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.1086 Acc: 0.9615\n",
            "val Loss: 0.3209 Acc: 0.9000\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.1137 Acc: 0.9744\n",
            "val Loss: 0.5193 Acc: 0.9000\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.0894 Acc: 0.9615\n",
            "val Loss: 0.2193 Acc: 0.9000\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.0401 Acc: 0.9872\n",
            "val Loss: 0.5253 Acc: 0.9000\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.0634 Acc: 0.9615\n",
            "val Loss: 0.7285 Acc: 0.8500\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.0388 Acc: 1.0000\n",
            "val Loss: 0.4617 Acc: 0.9000\n",
            "\n",
            "Best val Acc: 0.950000\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                dataloader = train_loader\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "                dataloader = val_loader\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                # Track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # Deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = model.state_dict()\n",
        "\n",
        "        print()\n",
        "\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-WcTKTnuhTky",
        "outputId": "6baac3a6-44f2-4298-b92d-37cbee7a3b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7619\n"
          ]
        }
      ],
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    test_acc = running_corrects.double() / len(test_loader.dataset)\n",
        "    print(f'Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Test the model\n",
        "test_model(model_ft, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEkzBxbehVoh"
      },
      "outputs": [],
      "source": [
        "torch.save(model_ft.state_dict(), '/content/drive/MyDrive/ddubuk/model_resnet50.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_roc_auc(model, dataloader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]  # 클래스 1에 대한 확률을 가져옵니다.\n",
        "\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "    # Flatten the lists\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "\n",
        "    # ROC 곡선과 AUC 계산\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # ROC 곡선 출력\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.0])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f'AUC Score: {roc_auc:.4f}')\n",
        "\n",
        "# 테스트 데이터에 대해 ROC 곡선과 AUC 계산\n",
        "calculate_roc_auc(model_ft, test_loader)"
      ],
      "metadata": {
        "id": "Un7BPAOM0h9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1UOV1xWVvbQWJHJvYcCSf2xQo_PKVwJup",
      "authorship_tag": "ABX9TyPcvFGqedjLpB/FgW2acLRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}