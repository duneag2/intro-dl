Question 1. How can I move to the directory I want using the library os & Linux Command?
I'd like to move to the directory name '/content/sample_data'.


Answer 1. 

import os

directory_path = '/content/sample_data'

os.chdir(directory_path)



///


Question 2. Please write a code that copies new.txt with a file name new3.txt.
Directory: /content/drive/MyDrive/intro-dl/afhq/new_folder



Answer 2. 


import shutil
import os

# Change directory to the target location
os.chdir('/content/drive/MyDrive/intro-dl/afhq/new_folder')

source_file = 'new.txt'
destination_file = 'new3.txt'

shutil.copy(source_file, destination_file)


///


Question 3. Compute L1/L2 Norm between matrix1 and matrix2 above.
Hint: https://pytorch.org/docs/stable/generated/torch.linalg.norm.html


Answer 3. 

import torch
import torch.nn.functional as F

matrix1 = torch.tensor([[1., 2.], [3., 4.]])
matrix2 = torch.tensor([[5., 6.], [7., 8.]])

# L1 norm
l1_norm = torch.linalg.norm(matrix1 - matrix2, ord=1)
print("L1 Norm between matrix1 and matrix2:", l1_norm.item())

# L2 norm
l2_norm = torch.linalg.norm(matrix1 - matrix2, ord=2)
print("L2 Norm between matrix1 and matrix2:", l2_norm.item())




# cosine similarity (with flattening the matrices)
cos_sim = F.cosine_similarity(matrix1.flatten(), matrix2.flatten(), dim=0)
print("Cosine Similarity between matrix1 and matrix2:", cos_sim.item())



///

Question 4. Please write a line-by-line explanation of the code above. (Simple MLP only)


# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms, datasets

# Check if CUDA (GPU) is available, else use CPU
if torch.cuda.is_available():
    DEVICE = torch.device('cuda')
else:
    DEVICE = torch.device('cpu')
print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)

# Define batch size and number of epochs
BATCH_SIZE = 32
EPOCHS = 10

# Download and prepare MNIST dataset for training and testing
# MNIST: popular dataset of handwritten digits
train_dataset = datasets.MNIST(root="../data/MNIST", train=True, download=True, transform=transforms.ToTensor())
test_dataset = datasets.MNIST(root="../data/MNIST", train=False, transform=transforms.ToTensor())

train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Define the architecture of the Multi-Layer Perceptron (MLP) model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 512)  # Fully connected layer 1
        self.fc2 = nn.Linear(512, 256)      # Fully connected layer 2
        self.fc3 = nn.Linear(256, 10)       # Fully connected layer 3 (output layer)

    def forward(self, x):
        x = x.view(-1, 28 * 28)  # Flatten the input image
        x = self.fc1(x)          # Pass through first fully connected layer
        x = F.sigmoid(x)         # Apply sigmoid activation function
        x = self.fc2(x)          # Pass through second fully connected layer
        x = F.sigmoid(x)         # Apply sigmoid activation function
        x = self.fc3(x)          # Pass through output layer
        x = F.log_softmax(x, dim=1)  # Apply log softmax for classification
        return x

# Create an instance of the model and move it to the appropriate device
model = Net().to(DEVICE)
# Define optimizer and loss function
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)
criterion = nn.CrossEntropyLoss()

# Define training function
def train(model, train_loader, optimizer, log_interval):
    model.train()  # Set the model to training mode
    for batch_idx, (image, label) in enumerate(train_loader):
        image = image.to(DEVICE)  # Move image to device
        label = label.to(DEVICE)  # Move label to device
        optimizer.zero_grad()     # Clear previous gradients
        output = model(image)     # Forward pass
        loss = criterion(output, label)  # Calculate loss
        loss.backward()           # Backpropagation
        optimizer.step()          # Update weights

        if batch_idx % log_interval == 0:
            print("Train Epoch: {} [{}/{} ({:.0f}%)]\tTrain Loss: {:.6f}".format(
                epoch, batch_idx * len(image),
                len(train_loader.dataset), 100. * batch_idx / len(train_loader),
                loss.item()))

# Define evaluation function
def evaluate(model, test_loader):
    model.eval()  # Set the model to evaluation mode
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for image, label in test_loader:
            image = image.to(DEVICE)  # Move image to device
            label = label.to(DEVICE)  # Move label to device
            output = model(image)     # Forward pass
            test_loss += criterion(output, label).item()  # Calculate loss
            prediction = output.max(1, keepdim=True)[1]  # Get predicted labels
            correct += prediction.eq(label.view_as(prediction)).sum().item()  # Count correct predictions

    test_loss /= (len(test_loader.dataset) / BATCH_SIZE)  # Average test loss
    test_accuracy = 100. * correct / len(test_loader.dataset)  # Calculate accuracy
    return test_loss, test_accuracy

# Training loop
for epoch in range(1, EPOCHS + 1):
    train(model, train_loader, optimizer, log_interval=200)  # Train the model
    test_loss, test_accuracy = evaluate(model, test_loader)   # Evaluate the model
    print("\n[EPOCH: {}], \tTest Loss: {:.4f}, \tTest Accuracy: {:.2f} % \n".format(
        epoch, test_loss, test_accuracy))

























