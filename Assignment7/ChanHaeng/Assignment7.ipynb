{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2024 Winter Introduction to Deep Learning\n","### Based on Prof. Oh's Youtube Lecture\n","https://youtube.com/playlist?list=PLvbUC2Zh5oJvByu9KL82bswYT2IKf0K1M\n","\n","> Assignment #7\n","\n","\n","*   Youtube Lecture #27-31\n","*   Written by Seungeun Lee"],"metadata":{"id":"YcXb0XLEvHns"}},{"cell_type":"markdown","source":["## 1. Recurrent Neural Network [RNN]\n","*     Reference. https://data-science.tistory.com/67"],"metadata":{"id":"qM2kM5G7BYZr"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"11Ih6xyA5NoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_size = 5\n","hidden_size = 8"],"metadata":{"id":"LF3Dn5b_5Ni2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# (batch_size, time_steps, input_size)\n","inputs = torch.Tensor(1, 10, 5)"],"metadata":{"id":"ipCaOFjQ7Y_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cell = nn.RNN(input_size, hidden_size, batch_first=True) # defines the RNN architecture\n","# batch_first = True -> indicates that the first dimension stands for the batch size\n","# if False, the input should be (10, 5), i.e. (time_steps, input_size), getting rid of the batch_size"],"metadata":{"id":"ftRUJDWQ7Y4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs, _status = cell(inputs)"],"metadata":{"id":"qmUmWqV-7Yye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs.shape) # hidden_state of every time step (8-dim hidden_state for 10 time steps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0XeLU8s7Ytd","outputId":"83696eeb-e8c8-4aa1-f4cd-60325776b53a","executionInfo":{"status":"ok","timestamp":1722756368172,"user_tz":-540,"elapsed":7,"user":{"displayName":"SEED","userId":"07195789800297982207"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 8])\n"]}]},{"cell_type":"code","source":["print(_status.shape) # hidden_state of the final layer only (8-dim hidden_state for 1 time step)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaFANjmp7Yn_","outputId":"8516ec9b-4c5b-408d-8f2e-6c5f30ae9072","executionInfo":{"status":"ok","timestamp":1722756368172,"user_tz":-540,"elapsed":6,"user":{"displayName":"SEED","userId":"07195789800297982207"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 8])\n"]}]},{"cell_type":"code","source":["# Deeper RNN\n","# (batch_size, time_steps, input_size)\n","inputs2 = torch.Tensor(1, 10, 5)\n","cell2 = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True) # num_layers = 2 -> deeper RNN (default: 1)\n","outputs2, _status2 = cell2(inputs2)\n","print(outputs2.shape) # hidden_state of every time step (8-dim hidden_state for 10 time steps) -> only returns the last layer\n","print(_status2.shape) # hidden_state of the final layer only (8-dim hidden_state for 1 time step) -> returns the output for all 2 layers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bx146Jg7YiU","outputId":"8a8bcc3f-7012-4963-c044-0de349d4a7a0","executionInfo":{"status":"ok","timestamp":1722756369376,"user_tz":-540,"elapsed":1208,"user":{"displayName":"SEED","userId":"07195789800297982207"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 8])\n","torch.Size([2, 1, 8])\n"]}]},{"cell_type":"code","source":["# Deeper RNN\n","# (batch_size, time_steps, input_size)\n","inputs3 = torch.Tensor(1, 10, 5)\n","cell3 = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 3, batch_first=True) # num_layers = 3 -> deeper RNN (default: 1)\n","outputs3, _status3 = cell3(inputs3)\n","print(outputs3.shape) # hidden_state of every time step (8-dim hidden_state for 10 time steps) -> only returns the last layer\n","print(_status3.shape) # hidden_state of the final layer only (8-dim hidden_state for 1 time step) -> returns the output for all 3 layers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivGBFM0F9BaB","outputId":"da416383-0958-4b0d-9462-8ce22d644a86","executionInfo":{"status":"ok","timestamp":1722756369376,"user_tz":-540,"elapsed":11,"user":{"displayName":"SEED","userId":"07195789800297982207"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 8])\n","torch.Size([3, 1, 8])\n"]}]},{"cell_type":"markdown","source":["## 2. Long Short Term Memory [LSTM]"],"metadata":{"id":"Qb5wK4jY3Aw9"}},{"cell_type":"code","source":["# Deeper LSTM -- we only need to change nn.RNN into nn.LSTM\n","# (batch_size, time_steps, input_size)\n","inputs4 = torch.Tensor(1, 10, 5)\n","cell4 = nn.LSTM(input_size = 5, hidden_size = 8, num_layers = 4, batch_first=True) # num_layers = 4 -> deeper LSTM (default: 1)\n","outputs4, (h4, c4) = cell4(inputs4)\n","print(outputs4.shape) # hidden_state of every time step (8-dim hidden_state for 10 time steps) -> only returns the last layer\n","print(h4.shape) # hidden_state of the final layer only (8-dim hidden_state for 1 time step) -> returns the output for all 4 layers\n","print(c4.shape) # cell state of the final layer only (8-dim hidden_state for 1 time step) -> returns the output for all 4 layers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FqZefh_T5OaX","outputId":"75004dea-965f-463f-b681-e99624a56462","executionInfo":{"status":"ok","timestamp":1722756369376,"user_tz":-540,"elapsed":10,"user":{"displayName":"SEED","userId":"07195789800297982207"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 10, 8])\n","torch.Size([4, 1, 8])\n","torch.Size([4, 1, 8])\n"]}]},{"cell_type":"markdown","source":["### Question 1.\n","### Write a code for LSTM having 10 internal layers (num_layers = 10) and change the input size into (5, 50, 5). Please stick to the format provided. Check if the size of the output, hidden, and cell state are calculated properly."],"metadata":{"id":"n64ndo75_Nfw"}},{"cell_type":"code","source":["inputs5 = torch.Tensor(5, 50, 5) # Changing batch size, time step\n","cell5 = nn.LSTM(input_size = 5, hidden_size = 8, num_layers = 10, batch_first=True) # 10 Layers\n","outputs5, (h5, c5) = cell5(inputs5)\n","print(outputs5.shape)\n","print(h5.shape)\n","print(c5.shape)"],"metadata":{"id":"IpVz1YapCZ5T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722756731466,"user_tz":-540,"elapsed":459,"user":{"displayName":"SEED","userId":"07195789800297982207"}},"outputId":"4c656da0-5a5c-451f-f54d-7339b444dc33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 50, 8])\n","torch.Size([10, 5, 8])\n","torch.Size([10, 5, 8])\n"]}]},{"cell_type":"markdown","source":["### Question 2. Describe the limiations of (Vanilla) RNN and how LSTM overcomes these limitations."],"metadata":{"id":"7GDml6Jn5xIQ"}},{"cell_type":"code","source":["# RNN은 정보가 시간이 지나며 과거 정보가 희석된다\n","# 그러나 최신 정보여도 부가적인 정보거나, 과거의 것이 핵심 정보일 수도 있다. (대표적으로 언어 처리)\n","# LSTM은 Hidden state외에도 Cell state를 도입해 과거의 일을 '잊는 정도'도 각각 다르게 하여 학습이 가능하다"],"metadata":{"id":"9rbYbX4uAig_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Gated Recurrent Unit [GRU]"],"metadata":{"id":"V9sYztYRAGNO"}},{"cell_type":"markdown","source":["### Question 3.\n","### Write a code for GRU having 5 internal layers (num_layers = 5) and change the input size into (3, 45, 7). (Change the input_size adequately) Please stick to the format provided. Check if the size of output and hidden state are calculated properly.\n","### Hint: https://pytorch.org/docs/stable/generated/torch.nn.GRU.html Its implementation is similar to that of RNN."],"metadata":{"id":"47Lz-iLAAWOS"}},{"cell_type":"code","source":["inputs6 = torch.Tensor(3, 45, 7) # Changing batch size, time step, Input size\n","cell6 = nn.GRU(input_size = 7, hidden_size = 8, num_layers = 5, batch_first=True) # 5 Layers, GRU\n","outputs6, h6 = cell6(inputs6)\n","print(outputs4.shape)\n","print(h6.shape)"],"metadata":{"id":"jqA_pfdaBN0F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722757247971,"user_tz":-540,"elapsed":364,"user":{"displayName":"SEED","userId":"07195789800297982207"}},"outputId":"df5ecc52-f230-4f60-c6a8-62ae2c67613c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 45, 8])\n","torch.Size([5, 3, 8])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wx026812BNuT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. On your Own"],"metadata":{"id":"seqJ1hGC3KXI"}},{"cell_type":"markdown","source":["### **Question 4. Choose one or two from the following documents:**\n","\n","\n","*    **News topic Classification with RNN:** https://glanceyes.com/entry/PyTorch%EB%A1%9C-RNN-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84%ED%95%B4%EB%B3%B4%EA%B8%B0-AG-NEWS-%EB%89%B4%EC%8A%A4-%EA%B8%B0%EC%82%AC-%EC%A3%BC%EC%A0%9C-%EB%B6%84%EB%A5%98\n","*    **NAVER Movie Review Classification with LSTM:** https://wikidocs.net/217687\n","*   **IMDB Review Classification with GRU:** https://wikidocs.net/217083\n","\n","#### Read it and run the whole code. Write a simplified explanation for each cell.\n"],"metadata":{"id":"_8C9KcjF3Ygs"}},{"cell_type":"markdown","source":["Note. https://wikidocs.net/book/2788 and https://wikidocs.net/book/2155 provide lots of interesting codes!"],"metadata":{"id":"BEl8LO2q4vr2"}},{"cell_type":"code","source":["!pip install torch\n","!pip install torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ti3U_koj2TOo","executionInfo":{"status":"ok","timestamp":1722866647266,"user_tz":-540,"elapsed":84880,"user":{"displayName":"SEED","userId":"07195789800297982207"}},"outputId":"13ac65ea-a900-4c00-f492-a709df0b0b68"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.6.20)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.7.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchtext\n","\n","from tqdm.notebook import tqdm_notebook\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","\n","# 출처: https://glanceyes.com/entry/PyTorch로-RNN-모델-구현해보기-AG-NEWS-뉴스-기사-주제-분류 [지그시:티스토리]"],"metadata":{"id":"VFCJ_YjpCn9g","executionInfo":{"status":"ok","timestamp":1722866700888,"user_tz":-540,"elapsed":4815,"user":{"displayName":"SEED","userId":"07195789800297982207"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a386b8e1-f2db-49a4-b2dc-6bad03c25e17"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n","/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n","Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n","  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"]}]},{"cell_type":"code","source":["data_dir = './data' # Directory\n","dataset = torchtext.datasets.AG_NEWS(root=data_dir, split='train') # News data load"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":166},"id":"tosdkYSsWnLd","executionInfo":{"status":"error","timestamp":1722866705206,"user_tz":-540,"elapsed":1016,"user":{"displayName":"SEED","userId":"07195789800297982207"}},"outputId":"6811b2aa-22a9-48c1-d525-e12c2c4303d4"},"execution_count":3,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'torchtext' has no attribute 'datasets'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-bc5a2f3d7874>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data'\u001b[0m \u001b[0;31m# Directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAG_NEWS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# News data load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'torchtext' has no attribute 'datasets'"]}]},{"cell_type":"markdown","source":["### The End."],"metadata":{"id":"spc48rNHQnYL"}},{"cell_type":"markdown","source":["##### Please upload your Colab file @Github https://github.com/duneag2/intro-dl/tree/main/Assignment7\n","\n","*   First, make your folder by your name (e.g. seungeun)\n","*   Then upload your \"Jupyter Notebook\" file under that directory\n","\n","###### Need Help?\n","\n","\n","\n","*   Please refer to this link https://yeko90.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC-colab%EC%BD%94%EB%9E%A9%EC%97%90%EC%84%9C-%EC%95%95%EC%B6%95%ED%8C%8C%EC%9D%BC-%ED%92%80%EA%B8%B0 OR\n","*   Just save your Jupyter Notebook (.ipynb) file in here (colab) and upload via 'Add file' - 'Upload files' https://nthree.tistory.com/60"],"metadata":{"id":"iMNBVkjiS7D9"}},{"cell_type":"code","source":[],"metadata":{"id":"XzVGuer0S9Oh"},"execution_count":null,"outputs":[]}]}