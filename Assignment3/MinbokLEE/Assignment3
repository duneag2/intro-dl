Question 1:


os.path.join function is used to specify directory within plt.imread function, which reads -
the image file and returns into NumPy arry.



Question 2:



autograd accumulates gradients into .grad attribute. 
It enables backpropagation, with using chain rule.



Question 3: 

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import warnings
from matplotlib.colors import ListedColormap
from mlxtend.plotting import plot_decision_regions
from pylab import rcParams
from sklearn.datasets import make_circles, make_moons
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
Imports necessary libraries such as Matplotlib, NumPy, Pandas, TensorFlow, and others.
Sets up some configurations for plotting and suppresses warnings.
Imports specific functions and classes required for creating datasets, splitting data, and building neural network models.


np.set_printoptions(suppress=True)
zero_one_colourmap = ListedColormap(('white', 'red'))
rcParams['figure.figsize'] = 14, 7
rcParams['axes.facecolor'] = '#383838'
Sets NumPy print options to suppress scientific notation.
Creates a color map for plotting.
Sets figure size and background color for plots.


X, y = make_circles(n_samples = 1000,
                    factor=0.85,
                    random_state=2021,
                    noise=0.1)
Generates a synthetic dataset of 1000 samples with two classes using the make_circles function from scikit-learn.


plt.scatter(X[:,0],X[:,1],
            c=y, s=100,
            cmap = zero_one_colourmap)
plt.show()
Plots the generated dataset using Matplotlib.


X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.33,
                                                    random_state=42)
Splits the dataset into training and testing sets using the train_test_split function from scikit-learn.


def get_model(inp_activation):
  model = Sequential()
  model.add(Dense(10,input_dim=2, activation=inp_activation))
  # More dense layers added with the specified activation function
  model.add(Dense(1, activation="sigmoid"))
  model.compile(loss='binary_crossentropy',
                optimizer='adam',
                metrics=['accuracy'])
  return model
Defines a function get_model to create a neural network model with specified activation function.
The model consists of a sequential stack of dense layers with 10 neurons each and an output layer with sigmoid activation.
Compiles the model with binary cross-entropy loss, Adam optimizer, and accuracy metric.


def change_in_weight_gradient(old_weight, new_weight, learning_rate):
  gradient = (old_weight - new_weight) / learning_rate
  pct_change_weight = abs(100 * (old_weight - new_weight) / old_weight)
  return gradient, pct_change_weight
Defines a function change_in_weight_gradient to calculate the change in weights and gradients.



Question 4:


